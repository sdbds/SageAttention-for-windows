å®˜æ–¹çš„ä»“åº“ï¼šhttps://github.com/sdbds/SageAttention-for-windows

å› ä¸ºå®˜æ–¹ä»“åº“æ²¡æœ‰é€‚é…ç§‹å¶æ•´åˆåŒ…V2çš„SageAttention2.2é¢„ç¼–è¯‘ç‰ˆæœ¬ã€‚ä¸ºäº†ç©Z-imageæˆ‘åªèƒ½è‡ªå·±ç¼–è¯‘ä¸€ä¸ªã€‚

æ•ˆæœè¿˜æ˜¯å¾ˆç»™åŠ›çš„ï¼ŒFP8æ¨¡å‹+FP16çš„seedvr2å‡º4Kå›¾åœ¨æˆ‘çš„4080S+32GRAMä¸‹ç”šè‡³å¯ä»¥æ§åˆ¶åœ¨50Så·¦å³ï¼ˆéé¦–æ¬¡å‡ºå›¾æ—¶ã€æ¯æ¬¡å¯åŠ¨çš„ç¬¬ä¸€å¼ å›¾ä¼šæ¯”è¾ƒæ…¢ï¼‰ã€‚

ä¸­é—´è¸©äº†å¾ˆå¤šå‘ï¼Œä»€ä¹ˆä¾èµ–åº“ç¼ºå¤±ã€å¶æ•´åˆåŒ…è‡ªå¸¦cu129è£…æœ€æ–°çš„13.0ç‰ˆæœ¬cuda Toolkitå°±ä¼šç¼–è¯‘æŠ¥é”™ã€Microsoft Visual C++ç¼ºå¤±ã€Tritonæœ€æ–°ç‰ˆ3.5ä¸åŒ¹é…torch2.8å¿…é¡»é™çº§3.4å¦åˆ™SageAttentionä¼šæŠ¥é”™ï¼ˆError running sage attention: Triton only support CUDA 10.0 or higher, but got CUDA version: 12.8, using pytorch attention instead.ï¼‰

å¥½ä¸å®¹æ˜“ç¼–è¯‘å‡ºæ¥äº†ï¼Œç‹¬ä¹ä¹ä¸å¦‚ä¼—ä¹ä¹ï¼Œä¹Ÿè®©æˆ‘è¸©çš„å‘æ›´æœ‰ä»·å€¼ã€‚

å‚è€ƒå®‰è£…æ•™ç¨‹ï¼š
å¦‚ä½•åœ¨æœ€æ–°ç§‹å¶æ•´åˆåŒ…(ComfyUI-aki-v2)ä¸­å®‰è£…SageAttentionæœ€æ–°ç‰ˆï¼ˆ2.2.0ï¼‰

æ³¨æ„ï¼šä»¥ä¸‹æ“ä½œå…¨éƒ¨åœ¨ç§‹å¶æ•´åˆåŒ…è‡ªå¸¦çš„CMDçª—å£ä¸­è¿è¡Œï¼å¯ä»¥å…å»é…ç½®ç¯å¢ƒå˜é‡å’Œè¢«ç³»ç»Ÿè‡ªå¸¦çš„pythonå¹²æ‰°çš„é£é™©ï¼

ç§‹å¶æ•´åˆåŒ…ä¸‹è½½åœ°å€ï¼šhttps://t.bilibili.com/1099726378314498050

Z-imageæ¨¡å‹å’Œæ¨èå·¥ä½œæµä¸‹è½½ï¼šhttps://www.bilibili.com/video/BV1pRSMBHERD

comfyuiåŸºç¡€æ•™ç¨‹å‚è€ƒï¼šhttps://www.bilibili.com/video/BV11pHtzoEsf/

ï¼ˆæœ€å¥½æŒ‰ç…§å·¥ä½œæµä½œè€…çš„æ•´åˆåŒ…é‡Œå®‰è£…çš„æ’ä»¶åˆ—è¡¨ï¼Œåœ¨ç§‹å¶V2é‡Œå†å®‰è£…ä¸€éï¼Œå·¥ä½œæµå°±ä¸ä¼šæŠ¥é”™äº†ï¼‰ï¼ˆä¸ºä»€ä¹ˆä¸ç”¨å·¥ä½œæµä½œè€…è‡ªå·±çš„æ•´åˆåŒ…å‘¢ï¼Ÿemmmå…¶å®ä¹Ÿæ˜¯å¯ä»¥çš„ ï¼Œä½†æ˜¯é‚£ä¸ªç§‹å¶æ•´åˆåŒ…ç‰ˆæœ¬æ¯”è¾ƒæ—§æ˜¯V1çš„ï¼‰ï¼ˆå®‰è£…å®Œè®°å¾—æ›´æ–°comfyuiåˆ°æœ€æ–°å¼€å‘ç‰ˆæ‰èƒ½è·‘ï¼‰

1.ç¡®è®¤pythonç‰ˆæœ¬å’ŒTorchç‰ˆæœ¬

åœ¨CMDä¸­è¾“å…¥ï¼šâ€œpython -m pip list | findstr torchâ€ã€‚å¦‚æœä½ çš„ç§‹å¶æ•´åˆåŒ…ç‰ˆæœ¬æ­£ç¡®ï¼Œåº”è¯¥å¯ä»¥çœ‹åˆ°ï¼š

open_clip_torch ï¼š3.2.0

torch ï¼š2.8.0+cu129

torchaudio ï¼š2.8.0+cu129

torchsdeï¼š 0.2.6

torchvision ï¼š0.23.0+cu129

è¾“å…¥ï¼špython --versionï¼Œç§‹å¶æ•´åˆåŒ…è‡ªå¸¦çš„pythonç‰ˆæœ¬åº”è¯¥æ˜¯cpython-312

å‡çº§å¼ºè¿«ç—‡å¯é€‰ï¼špython -m pip install --upgrade pip

2.å®‰è£…ä¾èµ–Triton 3.4

ï¼ˆTritonçš„æœ€æ–°ç‰ˆå·²ç»æ›´æ–°åˆ°3.5ï¼Œä½†æ˜¯å®˜æ–¹æ–‡æ¡£æ˜¾ç¤ºtorch 2.8.0åªèƒ½åŒ¹é…3.5ä¹‹å‰çš„ç‰ˆæœ¬ï¼Œæ‰€ä»¥ä¸è¦è¯•å›¾å®‰è£…æœ€æ–°ç‰ˆï¼Œå¦åˆ™ä¼šæŠ¥é”™Error running sage attention: Triton only support CUDA 10.0 or higher, but got CUDA version: 12.8, using pytorch attention instead.ï¼‰

å‘½ä»¤ï¼špython -m pip install -U "triton-windows<3.5"

å¦‚æœä½ ä¹‹å‰å®‰è£…äº†å…¶ä»–ç‰ˆæœ¬çš„tritonæˆ–è€…triton-windowséœ€è¦å…¨éƒ¨å¸è½½ï¼ˆé€šè¿‡python -m pip list | findstr tritonå¯ä»¥åˆ¤æ–­æ˜¯å¦å·²ç»å®‰è£…ï¼Œé»˜è®¤æƒ…å†µä¸‹æ˜¯ç©ºè¾“å‡ºï¼‰

python -m pip uninstall triton triton-windows

3.ä¸‹è½½æˆ‘ç¼–è¯‘å¥½çš„SageAttentionã€‚ï¼ˆåœ¨æœ¬ä»“åº“çš„releasesä¸­ï¼‰

æ³¨æ„ï¼Œæˆ‘ç¼–è¯‘çš„è¿™ä¸ªç‰ˆæœ¬ï¼Œä»…é€‚é…torch 2.8.0+cu129ã€python3.12çš„ç¯å¢ƒï¼ï¼ï¼ï¼ˆä¹Ÿå°±æ˜¯ä¸é€‚é…V1ç‰ˆæœ¬çš„æ•´åˆåŒ…ï¼‰

å®‰è£…æ–¹æ³•ï¼šæŠŠä¸‹è½½ä¸‹æ¥çš„è¿™ä¸ª .whl æ–‡ä»¶ï¼Œæ”¾åˆ° C:\AI\ComfyUI-aki-v2\ComfyUI è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œã€‚ï¼ˆæ³¨æ„ï¼šC:\AI\ComfyUI-aki-v2\ è¦æ›¿æ¢ä¸ºä½ ç§‹å¶æ•´åˆåŒ…çš„å®‰è£…ä½ç½®ï¼ï¼‰

åœ¨CMDä¸­è¾“å…¥ï¼špython -m pip install sageattention-2.2.0-cp312-cp312-win_amd64.whl

4.éªŒè¯å®‰è£…

python -m pip list | findstr SageAttention

æ­£å¸¸æƒ…å†µä¸‹ï¼Œç¬¬ä¸€æ¬¡è·‘å›¾æ¯”è¾ƒæ…¢ï¼Œç­‰ç³»ç»Ÿç¼–è¯‘ å¥½CUDA æ ¸å°±èƒ½æ­£å¸¸åŠ é€Ÿäº†ã€‚

ç–‘éš¾æ‚ç—‡æ’è§£ï¼š

1.å®‰è£…è¿‡ç¨‹ä¸­å‡ºç°æ˜æ˜¾ ERROR / Failed to buildï¼Œè¯·æŠŠå…¨éƒ¨æ—¥å¿—å¤åˆ¶ä¸‹æ¥ä¸¢ç»™AIï¼ˆå›½äº§çš„qwen3-maxæ¨èï¼‰ï¼Œè®©AIåˆ¤æ–­ç¼ºäº†ä»€ä¹ˆä¾èµ–å¹¶ç»™å‡ºå®‰è£…å‘½ä»¤ã€‚

2.ç¼–è¯‘å¥½çš„ .whl é€šå¸¸ä¸å†éœ€è¦æœ¬åœ° CUDA Toolkit ä»¥åŠ Visual Studio.
å¦‚æœå‡ºç°æŠ¥é”™ã€‚è¯·åˆ° https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html  æŸ¥è¯¢ä½ æ˜¾å¡é©±åŠ¨å¯¹åº”çš„CUDA Toolkitçš„ç‰ˆæœ¬ã€‚ï¼ˆæ˜¾å¡é©±åŠ¨ç‰ˆæœ¬å¯åœ¨NVDIAæ§åˆ¶é¢æ¿æŸ¥çœ‹ï¼‰
ç„¶ååˆ° https://developer.nvidia.com/cuda-toolkit-archive   ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ã€‚

ç‰¹åˆ«æé†’ï¼šç§‹å¶æ•´åˆåŒ…çš„cu129æœ€é«˜åªèƒ½å®‰è£…åˆ°CUDA Toolkit 12.9 Update 1ï¼å®‰è£…æœ€æ–°ç‰ˆæœ¬ä¼šæŠ¥é”™ï¼å¦‚éœ€å®‰è£…Visual Studioï¼Œéœ€è¦å‹¾é€‰â€œWindows 10 SDKâ€ æˆ–è€… â€œWindows 11 SDKâ€ï¼ˆé€šå¸¸åœ¨â€œä½¿ç”¨ C++ çš„æ¡Œé¢å¼€å‘â€é‚£ä¸€æ çš„å³ä¾§è¯¦ç»†åˆ—è¡¨é‡Œï¼‰ã€‚

--- -->

# SageAttention
<!-- We are continuously updating more features. You could **Star** and **Watch** our repository to stay updated.

--- -->
This repository provides the official implementation of SageAttention, SageAttention2, and SageAttention2++, which achieve surprising speedup on most GPUs without lossing accuracy across all models in a plug-and-play way.

**SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration**  
Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen  
Paper: https://arxiv.org/abs/2410.02367  

**SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization**  
Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen  
Paper: https://arxiv.org/abs/2411.10958  

**SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training**  
Jintao Zhang, Jia Wei, Haoxu Wang, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Kai Jiang, Jun Zhu, Jianfei Chen  
Paper: https://arxiv.org/abs/2505.11594  


![Local Image](./assets/2.png)
*Note: [SageAttention2++](https://arxiv.org/pdf/2505.21136) achieves higher speed while maintaining the same accuracy performance.*

## Current Features
<!-- This is a beta release of SageAttention2. We welcome any feedback on accuracy, performance issues, bugs, feature requests, or suggestions. Please feel free to open an issue or launch a pull request! -->

+ Optmized kernels for **Ampere, Ada and Hopper GPUs.**
+ INT8 quantization and smoothing for $QK^\top$ with support for varying granularities.
+ FP8 quantization for $PV$, and FP16 accumulator for FP8/FP16 $PV$.
+ Two-level accumulation strategy for $PV$ to improve accuracy in FP8 MMA and WGMMA.
+ Support `torch.compile` with non-cudagraphs mode and distributed inference.


## Project Updates
- [2025-09-27]: ğŸ‰ [SageAttention3](https://arxiv.org/abs/2505.11594) is accepted by NeurIPS 2025 as a **Spotlight** paper! 
- [2025-09-27]: The code of [SageAttention3](https://arxiv.org/abs/2505.11594) is released in this repository at  [sageattention3_blackwell](./sageattention3_blackwell/). We would still greatly appreciate it if you could take a moment to fill out the Form in [Huggingface](https://huggingface.co/jt-zhang/SageAttention3). Please note that since SageAttention2 is more accurate, we still recommend using SageAttention2 for precision-sensitive applications.
- [2025-07-01]: The code of [SageAttention2++](https://arxiv.org/pdf/2505.21136) is released in this repository. We would still greatly appreciate it if you could take a moment to fill out the Form in [Huggingface](https://huggingface.co/jt-zhang/SageAttention2_plus). Thank you very much!

![Local Image](./assets/5090_sageattn2++.png)

![Local Image](./assets/4090_sageattn2++.png)

- [2025-06-19]: [Sparse SageAttention1 API](https://github.com/jt-zhang/Sparse_SageAttention_API) and [Sparse SageAttention2 API](https://github.com/thu-ml/SpargeAttn) can compute attention with any block sparse pattern very fast.
- [2025-05-02]: ğŸ‰SageAttention2 and [SpargeAttn](https://github.com/thu-ml/SpargeAttn) are accepted by ICML 2025! 
- [2025-02-25]: ğŸ”¥ We release [SpargeAttn](https://github.com/thu-ml/SpargeAttn), a sparse attention based on SageAttention2, which could acclerate any model without training.
- [2025-02-15]: ğŸ”¥ The compilation code is updated to support RTX5090! On RTX5090, SageAttention reaches 560T, 2.7x faster than FlashAttention2!
- [2025-01-28]: ğŸ”¥âš¡SageAttention is now available on Hopper GPUs (H100, H800, H20)! It matches the speed of FlashAttention3-FP8 but offers **much better accuracy!**

| **FlashAttention2** | **FlashAttention3** | **FlashAttention3-FP8** | **SageAttention** |
|----------------------|----------------------|----------------------|----------------------|
| ![FlashAttention2](assets/cogvideox1.5_fa2_example.gif) | ![FlashAttention3](assets/cogvideox1.5_fa3_example.gif)  | ![FlashAttention3-FP8](assets/cogvideox1.5_fa3fp8_example.gif) | ![SageAttention](assets/cogvideox1.5_sage_example.gif) |
| **25'34''** | **17'32''** | **12'14''** | **12'07''** |

*Results for [CogVideoX1.5-5B](https://huggingface.co/THUDM/CogVideoX1.5-5B) on NVIDIA H20 GPU*

![Local Image](./assets/H100_hd128.png)

![Local Image](./assets/H20_hd128.png)

- [2025-01-24]: ğŸ‰SageAttention is accepted by ICLR 2025! 
- [2024-12-20]: ğŸ”¥Update the [SageAttention2 Paper](https://arxiv.org/abs/2411.10958).   

- [2024-12-20]: ğŸ”¥Release SageAttention 2.0.1 Beta! In this version, we introduce a new feature: per-thread quantization, which offers finer granularity while maintaining hardware efficiency.
- [2024-11-21]: ğŸ”¥SageAttention 2.0.0 beta is released! Now SageAttention has measured speedup on L20, L40, A100, A800, and A6000, RTX3090 and RTX4090.
- [2024-11-12]: Support for `sageattn_varlen` is available now.
- [2024-11-11]: Support for different sequence lengths between `q` and `k,v`,  `(batch_size, head_num, seq_len, head_dim)` or `(batch_size, seq_len, head_num, head_dim)` input shapes, and `group-query attention` is available now.


## Installation
### Base environment
+ `python>=3.9`   , `torch>=2.3.0`  , `triton>=3.0.0` 
- `CUDA`:
  + `>=12.8` for Blackwell or SageAttention2++
  + `>=12.4` for fp8 support on Ada
  + `>=12.3` for fp8 support on Hopper
  + `>=12.0` for Ampere
+ `flash-attn` for benchmarking

### Install Package

For SageAttention V1 in Triton (slower than SageAttention V2/V2++/V3), refer to [SageAttention-1](https://github.com/thu-ml/SageAttention/tree/sageattention-1) branch and install using pip: `pip install sageattention==1.0.6`

To use SageAttention 2.2.0 (containing SageAttention2++), you can install using pip:
```
pip install sageattention==2.2.0 --no-build-isolation
```

**Or** you can compile from source:
```
git clone https://github.com/thu-ml/SageAttention.git
cd SageAttention 
export EXT_PARALLEL=4 NVCC_APPEND_FLAGS="--threads 8" MAX_JOBS=32 # Optional
python setup.py install
```

To benchmark the speed against FlashAttention3, please compile FlashAttention3 from source:
```
git clone https://github.com/Dao-AILab/flash-attention.git --recursive
git checkout b7d29fb3b79f0b78b1c369a52aaa6628dabfb0d7 # 2.7.2 release
cd hopper
python setup.py install
```

## How to Use
```python
from sageattention import sageattn
attn_output = sageattn(q, k, v, tensor_layout="HND", is_causal=False)
```
+ `q, k, v` are **FP16/BF16** dtype with the shape `(batch_size, head_num, seq_len, head_dim)` using default `tensor_layout="HND"`. For shape `(batch_size, seq_len, head_num, head_dim)`, set `tensor_layout="NHD"`. 
+ `is_causal` determines the use of a causal mask.

### Available APIs:
+ `sageattn`: Automatically selects the optimal kernel based on the GPU to achieve a good performance-accuracy trade-off.
+ `sageattn_qk_int8_pv_fp16_triton`: INT8 quantization for $QK^\top$ and FP16 for $PV$ using Triton backend.
+ `sageattn_qk_int8_pv_fp16_cuda`: INT8 quantization for $QK^\top$ and FP16 for $PV$ using CUDA backend.
+ `sageattn_qk_int8_pv_fp8_cuda`: INT8 quantization for $QK^\top$ and FP8 for $PV$ using CUDA backend. (Note that setting `pv_accum_dtype=fp32+fp16` corresponds to SageAttention2++.)
+ `sageattn_qk_int8_pv_fp8_cuda_sm90`: INT8 quantization for $QK^\top$ and FP8 for $PV$ using CUDA backend, specifically optimized for Hopper GPUs.
+ `sageattn_varlen`: INT8 quantization for $QK^\top$ and FP16 for $PV$ using Triton backend. Support for varying sequence lengths within the same batch.

For optimal speed and accuracy performance on custom devices and models, we strongly recommend referring to the [this file](./sageattention/core.py) for detailed guidance.

> **Note:**
Support for different sequence lengths between `q` and `k,v` and `group-query attention` is available.


### Plug-and-play Example

We can replace `scaled_dot_product_attention` easily. 
We will take [CogvideoX](https://huggingface.co/THUDM/CogVideoX-2b) as an example:

Add the following codes and run
```diff
import torch.nn.functional as F

+ from sageattention import sageattn
+ F.scaled_dot_product_attention = sageattn

```

Specifically,

```bash
cd example
python cogvideox-2b.py --compile --attention_type sage
```

**You can get a lossless video in** `./example` **faster than by using** `python cogvideox-2b.py --compile`. More examples and guidance can be found under the `example/` directory.

> **Note:** Not all models works with `F.scaled_dot_product_attention = sageattn`. Technically, you should replace the original Attention by modifying the `Attention Class` of the target model. For image and video models, we suggest only replacing the attention in DiT (see `example/mochi.py` for detail).

### Kernel Benchmarking
We provide a benchmarking script to compare the speed of different kernels including SageAttention, FlashAttention2 and FlashAttention3. Please refer to the `benchmark/` directory for more details.
 
## Performance
### Speed of Kernels

`8+8` means the kernel with INT8 quantization for $QK^\top$ and FP8 quantization for $PV$. `8+16` uses FP16 with FP16 accumulator for $PV$.

![Local Image](./assets/5090_sageattn2++.png)

![Local Image](./assets/4090_sageattn2++.png)

![Local Image](./assets/4090_hd128.png)

![Local Image](./assets/L20_hd128.png)

![Local Image](./assets/H100_hd128.png)

![Local Image](./assets/H20_hd128.png)

![Local Image](./assets/A100_hd128.png)

![Local Image](./assets/3090_hd128.png)

> **Note:** The TOPS results refer only to the Attention Kernel, excluding the quantization and smoothing.

### End-to-end Performance
#### **End-to-End Accuracy:**

![Local Image](./assets/22.png)

![Local Image](./assets/23.png)

![Local Image](./assets/24.png)

![Local Image](./assets/25.png)

#### **End-to-End Speedup:**

![Local Image](./assets/26.png)
*Note: SageAttention2++ achieves higher speed.*

## Citation
**If you use this code or find our work valuable, please cite:**
```
@inproceedings{zhang2025sageattention,
  title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, 
  author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}
@inproceedings{zhang2024sageattention2,
  title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization},
  author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}
@article{zhang2025sageattention2++,
  title={Sageattention2++: A more efficient implementation of sageattention2},
  author={Zhang, Jintao and Xu, Xiaoming and Wei, Jia and Huang, Haofeng and Zhang, Pengle and Xiang, Chendong and Zhu, Jun and Chen, Jianfei},
  journal={arXiv preprint arXiv:2505.21136},
  year={2025}
}
@article{zhang2025sageattention3,
  title={SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training},
  author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Xu, Xiaoming and Huang, Haofeng and Wang, Haoxu and Jiang, Kai and Zhu, Jun and Chen, Jianfei},
  journal={arXiv preprint arXiv:2505.11594},
  year={2025}
}
```
